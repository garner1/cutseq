{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from scipy.sparse import *\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparseload(file):\n",
    "    f = open(file)\n",
    "    h = f.readline()\n",
    "    col = []\n",
    "    data = []\n",
    "    chrom = []\n",
    "    for l in f:\n",
    "        l = l.strip()\n",
    "        chrom.append(l.split('\\t')[0])\n",
    "        col.append(int(l.split('\\t')[1]))\n",
    "        data.append(int(float(l.split('\\t')[2])))\n",
    "    col = np.array(col)\n",
    "    data = np.array(data)\n",
    "    chrom = np.array(chrom)\n",
    "    f.close()\n",
    "    return chrom,col,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Experiment is one of: xz_10_15, xz_13, xz_14, xz_13_14 '''\n",
    "def structure_genome(resolution):\n",
    "    f = open('/home/garner1/Work/pipelines/data/relevant-chr-size_list.txt')\n",
    "    base = 0\n",
    "    positionlist = []\n",
    "    chromlist = []\n",
    "    border = []\n",
    "    for l in f.readlines():\n",
    "        l = l.strip()\n",
    "        chrom = l.split('\\t')[0]\n",
    "        size = l.split('\\t')[1]\n",
    "        end = base + int(size)/resolution\n",
    "        border.append(end)\n",
    "        array = arange(base, end)\n",
    "        positionlist.extend(array)\n",
    "        chromlist.extend([chrom]*len(array))\n",
    "        base = base + int(size)/resolution\n",
    "    f.close()\n",
    "    return positionlist,chromlist,border\n",
    "# positionlist,chromlist,border = structure_genome(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(experiment,resolution):\n",
    "    regions = os.listdir('/home/garner1/Work/dataset/restseq/data/'+str(experiment)+'/'+str(resolution))\n",
    "    length = 0\n",
    "    row_list = []\n",
    "    index0_list = []\n",
    "    index1_list = []\n",
    "    index2_list = []\n",
    "    index3_list = []\n",
    "    for region in sorted(regions):\n",
    "        filename = '/home/garner1/Work/dataset/restseq/data/'+str(experiment)+'/'+str(resolution)+'/'+region\n",
    "        if experiment=='xz_13' or experiment=='xz_14':\n",
    "            index0 = region[0]\n",
    "            index1 = region[2]\n",
    "            index0_list.append(index0)\n",
    "            index1_list.append(index1)\n",
    "        if experiment=='xz_10_15':\n",
    "            index0 = region[0]\n",
    "            index1 = region[2]\n",
    "            if len(region[3:len(region)-4])==3:\n",
    "                index2 = region[3:len(region)-4][0]\n",
    "                index3 = region[3:len(region)-4][2]\n",
    "            if len(region[3:len(region)-4])==2:\n",
    "                index2 = 'A'\n",
    "                index3 = region[3:len(region)-4][1]\n",
    "            index0_list.append(index0)\n",
    "            index1_list.append(index1)\n",
    "            index2_list.append(index2)\n",
    "            index3_list.append(index3)\n",
    "        if experiment=='xz_13_14':\n",
    "            index0 = region[:4]\n",
    "            index1 = region[5]\n",
    "            index2 = region[7]\n",
    "            index0_list.append(index0)\n",
    "            index1_list.append(index1)\n",
    "            index2_list.append(index2)\n",
    "        chrom, col, data = sparseload(filename)\n",
    "        row = np.zeros(len(col))\n",
    "        vec_sparse = csc_matrix((data,(row,col-1)), shape=(1,len(positionlist)))\n",
    "        row_list.append(vec_sparse.toarray())\n",
    "    X = np.vstack( row_list )\n",
    "    return X,index0_list,index1_list,index2_list,index3_list\n",
    "# X,index0_list,index1_list,index2_list,index3_list = load_data('xz_13',10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_RCdf(X,experiment,chromlist,positionlist,groupby,RC_threshold):\n",
    "    arrays = [chromlist, positionlist]\n",
    "    columns = pd.MultiIndex.from_arrays(arrays, names=['chromosome', 'bin'])\n",
    "    if experiment=='xz_10_15':\n",
    "        row = pd.MultiIndex.from_arrays([index0_list,index1_list,index2_list,index3_list],\\\n",
    "                                    names=['patient','type','set','fragment'])\n",
    "    if experiment=='xz_13_14':\n",
    "        row = pd.MultiIndex.from_arrays([index0_list,index1_list,index2_list],\\\n",
    "                                    names=['experiment','row','column'])\n",
    "    if experiment=='xz_13' or experiment=='xz_14':\n",
    "        row = pd.MultiIndex.from_arrays([index0_list,index1_list],\\\n",
    "                                    names=['row','column'])\n",
    "    X_df = pd.DataFrame(data=X,index=row,columns=columns)\n",
    "    '''\n",
    "    Remove rows with 0 read counts\n",
    "    '''\n",
    "    X_df = X_df.loc[~(X_df==0).all(axis=1)]\n",
    "\n",
    "    if groupby=='10-15-reduced':\n",
    "        X_red_df = X_df.groupby(level=['patient','type','set']).sum()\n",
    "    if groupby=='13-14-reduced':\n",
    "        X_red_df = X_df.groupby(level=['row','column']).sum()\n",
    "    if groupby=='none':\n",
    "        X_red_df = X_df\n",
    "    '''\n",
    "    Set zero columns\n",
    "    '''\n",
    "    mask = X==0\n",
    "    row_sum = (~mask).sum(axis=0)\n",
    "    mask = row_sum==0\n",
    "    zero_col = np.arange(X.shape[1])[mask]\n",
    "    '''\n",
    "    Remove columns with 0 read counts\n",
    "    '''\n",
    "    X_red_df = X_red_df[X_red_df.columns[(X_red_df != 0).any()]]\n",
    "    '''\n",
    "    Remove rows with read counts less than threshold\n",
    "    '''\n",
    "    X_red_df = X_red_df.loc[~(X_red_df.sum(axis=1)<RC_threshold)]\n",
    "    '''\n",
    "    Define the data matrix\n",
    "    '''\n",
    "    X = X_red_df.as_matrix()\n",
    "    return X_df,X_red_df,X,zero_col\n",
    "# X_df,X_red_df,X,zero_col = load_RCdf(X,'xz_13',chromlist,positionlist,'none',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiScaler] Initial log residual value = 14.039364\n",
      "[BiScaler] Iter 1: log residual = 4.014279, log improvement ratio=10.025085\n",
      "[BiScaler] Iter 2: log residual = 3.281676, log improvement ratio=0.732603\n",
      "[BiScaler] Iter 3: log residual = 2.003150, log improvement ratio=1.278526\n",
      "[BiScaler] Iter 4: log residual = 0.399320, log improvement ratio=1.603830\n",
      "[BiScaler] Iter 5: log residual = -1.218663, log improvement ratio=1.617983\n",
      "[BiScaler] Iter 6: log residual = -2.786118, log improvement ratio=1.567455\n",
      "[BiScaler] Iter 7: log residual = -4.291910, log improvement ratio=1.505792\n",
      "[BiScaler] Iter 8: log residual = -5.748049, log improvement ratio=1.456139\n",
      "[BiScaler] Iter 9: log residual = -7.169094, log improvement ratio=1.421045\n",
      "[BiScaler] Iter 10: log residual = -8.566925, log improvement ratio=1.397831\n",
      "[BiScaler] Iter 11: log residual = -9.949578, log improvement ratio=1.382653\n",
      "[BiScaler] Iter 12: log residual = -11.322263, log improvement ratio=1.372685\n",
      "[BiScaler] Iter 13: log residual = -12.688315, log improvement ratio=1.366052\n",
      "[BiScaler] Iter 14: log residual = -14.049888, log improvement ratio=1.361573\n",
      "[BiScaler] Iter 15: log residual = -15.408394, log improvement ratio=1.358506\n",
      "[BiScaler] Iter 16: log residual = -16.764774, log improvement ratio=1.356380\n",
      "[BiScaler] Iter 17: log residual = -18.119664, log improvement ratio=1.354890\n",
      "[BiScaler] Iter 18: log residual = -19.473501, log improvement ratio=1.353837\n",
      "[BiScaler] Iter 19: log residual = -20.826589, log improvement ratio=1.353088\n",
      "[BiScaler] Iter 20: log residual = -22.179141, log improvement ratio=1.352552\n",
      "[BiScaler] Iter 21: log residual = -23.531308, log improvement ratio=1.352167\n",
      "[BiScaler] Iter 22: log residual = -24.883198, log improvement ratio=1.351890\n",
      "[BiScaler] Iter 23: log residual = -26.234887, log improvement ratio=1.351690\n",
      "[BiScaler] Iter 24: log residual = -27.586432, log improvement ratio=1.351545\n",
      "[BiScaler] Iter 25: log residual = -28.937871, log improvement ratio=1.351440\n",
      "[BiScaler] Iter 26: log residual = -30.289235, log improvement ratio=1.351364\n",
      "[BiScaler] Iter 27: log residual = -31.640543, log improvement ratio=1.351308\n",
      "[BiScaler] Iter 28: log residual = -32.991812, log improvement ratio=1.351268\n",
      "[BiScaler] Iter 29: log residual = -34.343051, log improvement ratio=1.351239\n",
      "[BiScaler] Iter 30: log residual = -35.694269, log improvement ratio=1.351218\n",
      "[BiScaler] Iter 31: log residual = -37.045472, log improvement ratio=1.351203\n",
      "[BiScaler] Iter 32: log residual = -38.396664, log improvement ratio=1.351192\n",
      "[BiScaler] Iter 33: log residual = -39.747846, log improvement ratio=1.351183\n",
      "[BiScaler] Iter 34: log residual = -41.099023, log improvement ratio=1.351177\n",
      "[BiScaler] Iter 35: log residual = -42.450199, log improvement ratio=1.351175\n",
      "[BiScaler] Iter 36: log residual = -43.801366, log improvement ratio=1.351167\n",
      "[BiScaler] Iter 37: log residual = -45.152511, log improvement ratio=1.351145\n",
      "[BiScaler] Iter 38: log residual = -46.503685, log improvement ratio=1.351175\n",
      "[BiScaler] Iter 39: log residual = -47.854708, log improvement ratio=1.351023\n",
      "[BiScaler] Iter 40: log residual = -49.206152, log improvement ratio=1.351444\n",
      "[BiScaler] Iter 41: log residual = -50.557466, log improvement ratio=1.351314\n",
      "[BiScaler] Iter 42: log residual = -51.908282, log improvement ratio=1.350816\n",
      "[BiScaler] Iter 43: log residual = -53.259429, log improvement ratio=1.351147\n",
      "[BiScaler] Iter 44: log residual = -54.609915, log improvement ratio=1.350486\n",
      "[BiScaler] Iter 45: log residual = -55.956008, log improvement ratio=1.346093\n",
      "[BiScaler] Iter 46: log residual = -57.296593, log improvement ratio=1.340585\n",
      "[BiScaler] Iter 47: log residual = -58.654004, log improvement ratio=1.357411\n",
      "[BiScaler] Iter 48: log residual = -59.967628, log improvement ratio=1.313624\n",
      "[BiScaler] Iter 49: log residual = -61.272484, log improvement ratio=1.304856\n",
      "[BiScaler] Iter 50: log residual = -62.142800, log improvement ratio=0.870316\n",
      "[BiScaler] Iter 51: log residual = -62.744965, log improvement ratio=0.602165\n",
      "[BiScaler] Iter 52: log residual = -62.585939, log improvement ratio=-0.159026\n"
     ]
    }
   ],
   "source": [
    "def impute(X,zero_col):\n",
    "    X_incomplete = X.copy()\n",
    "    X_incomplete = X_incomplete*1.0\n",
    "    X_incomplete[X_incomplete==0] = np.nan\n",
    "    softImpute = SoftImpute(max_iters=100,n_power_iterations=3,min_value=0,verbose=False)\n",
    "    # # simultaneously normalizes the rows and columns of your observed data,\n",
    "    # # sometimes useful for low-rank imputation methods\n",
    "    biscaler = BiScaler()\n",
    "    # # rescale both rows and columns to have zero mean and unit variance\n",
    "    X_incomplete_normalized = biscaler.fit_transform(X_incomplete)\n",
    "    X_filled_softimpute_normalized = softImpute.complete(X_incomplete_normalized)\n",
    "    X_filled_softimpute = biscaler.inverse_transform(X_filled_softimpute_normalized)\n",
    "    X_filled_softimpute = softImpute.complete(X_incomplete)\n",
    "    '''\n",
    "    Re-insert the zeros cols\n",
    "    '''\n",
    "    zero_col = zero_col[:-2]\n",
    "    X_filled_softimpute = np.insert(X_filled_softimpute,zero_col,0,axis=1) \n",
    "    return X_filled_softimpute\n",
    "# X_filled_softimpute = impute(X,zero_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Normalize rows to 1\n",
    "'''\n",
    "def normalize(X_filled_softimpute):\n",
    "    col_sum = X_filled_softimpute.sum(axis=1)\n",
    "    X_filled_normalized = np.zeros(X_filled_softimpute.shape)\n",
    "    for ind in range(X_filled_softimpute.shape[0]):\n",
    "        X_filled_normalized[ind,:] = X_filled_softimpute[ind,:]*1.0/col_sum[ind]\n",
    "    X_filled_normalized_df = pd.DataFrame(data=X_filled_normalized,index=X_red_df.index)\n",
    "    X_filled_df = pd.DataFrame(data=X_filled_softimpute,index=X_red_df.index)\n",
    "    return X_filled_normalized_df,X_filled_df\n",
    "# X_filled_normalized_df,X_filled_df = normalize(X_filled_softimpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shannonDF(X_filled_df):\n",
    "    index = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    columns = ['1','2','3','4','5','6','7']\n",
    "    shannon_df_xz1314 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    shannon_df_xz13 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    shannon_df_xz14 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    shannon = []\n",
    "    df = X_filled_df\n",
    "    for i in df.index:\n",
    "        shannon.append( entropy(df.ix[i]) )\n",
    "    for i in range(len(df.index.values)):\n",
    "        if df.index.values[i][0]=='xz13':\n",
    "            shannon_df_xz13.ix[df.index.values[i][1],df.index.values[i][2]] = shannon[i]\n",
    "        if df.index.values[i][0]=='xz14':\n",
    "            shannon_df_xz14.ix[df.index.values[i][1],df.index.values[i][2]] = shannon[i]\n",
    "        if df.index.values[i][0]!='xz13' and df.index.values[i][0]!='xz14':\n",
    "            shannon_df_xz1314.ix[df.index.values[i][0],df.index.values[i][1]] = shannon[i]\n",
    "    return shannon_df_xz1314,shannon_df_xz13,shannon_df_xz14\n",
    "# shannon_df_xz1314,shannon_df_xz13,shannon_df_xz14 = shannonDF(X_filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readcountDF(X_filled_df):\n",
    "    index = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    columns = ['1','2','3','4','5','6','7']\n",
    "    totalRC_df_xz1314 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    totalRC_df_xz13 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    totalRC_df_xz14 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    totalRC = []\n",
    "    df = X_filled_df\n",
    "    for i in df.index:\n",
    "        totalRC.append( df.ix[i].sum(axis=0) )\n",
    "    for i in range(len(df.index.values)):\n",
    "        if df.index.values[i][0]=='xz13':\n",
    "            totalRC_df_xz13.ix[df.index.values[i][1],df.index.values[i][2]] = totalRC[i]\n",
    "        if df.index.values[i][0]=='xz14':\n",
    "            totalRC_df_xz14.ix[df.index.values[i][1],df.index.values[i][2]] = totalRC[i]\n",
    "        if df.index.values[i][0]!='xz13' and df.index.values[i][0]!='xz14':\n",
    "            totalRC_df_xz1314.ix[df.index.values[i][0],df.index.values[i][1]] = totalRC[i]\n",
    "    return totalRC_df_xz1314,totalRC_df_xz13,totalRC_df_xz14\n",
    "# totalRC_df_xz1314,totalRC_df_xz13,totalRC_df_xz14 = readcountDF(X_filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Distance between regions\n",
    "'''\n",
    "def NNdistDF_1314(X_filled_df,shannon_df_xz1314):\n",
    "    index = ['A','a-b','B','b-c','C','c-d','D','d-e','E','e-f','F','f-g','G','g-h','H','h-i','I','i-j','J']\n",
    "    columns = ['1','1-2','2','2-3','3','3-4','4','4-5','5','5-6','6','6-7','7']\n",
    "    delta_df_xz1314 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    df = X_filled_df\n",
    "    rr = 0\n",
    "    for row in range(len(shannon_df_xz1314.ix[:,0])-1):\n",
    "        cc = 1\n",
    "        for col in range(len(shannon_df_xz1314.ix[0,:])-1):\n",
    "            if totalRC_df_xz1314.ix[row,col]!=0 and totalRC_df_xz1314.ix[row,col+1]!=0:\n",
    "                ind1 = str(shannon_df_xz1314.index.values[row])\n",
    "                ind2 = str(shannon_df_xz1314.columns.values[col])\n",
    "                ind3 = str(shannon_df_xz1314.columns.values[col+1])\n",
    "                delta_df_xz1314.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind1,ind2],df.ix[ind1,ind3])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    rr = 1\n",
    "    for row in range(len(shannon_df_xz1314.ix[:,0])-1):\n",
    "        cc = 0\n",
    "        for col in range(len(shannon_df_xz1314.ix[0,:])-1):\n",
    "            if totalRC_df_xz1314.ix[row,col]!=0 and totalRC_df_xz1314.ix[row+1,col]!=0:\n",
    "                ind1 = str(shannon_df_xz1314.index.values[row])\n",
    "                ind2 = str(shannon_df_xz1314.columns.values[col])\n",
    "                ind3 = str(shannon_df_xz1314.index.values[row+1])\n",
    "                delta_df_xz1314.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind1,ind2],df.ix[ind3,ind2])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    return delta_df_xz1314\n",
    "# delta_df_xz1314 = NNdistDF_1314(X_filled_df,shannon_df_xz1314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fluctuations with respect to the average\n",
    "'''\n",
    "def fluctuationDF_1314(X_filled_df,X_filled_softimpute):\n",
    "    mean = X_filled_softimpute.sum(axis=0)\n",
    "    mean = mean*1.0/mean.sum()\n",
    "    index = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    columns = ['1','2','3','4','5','6','7']\n",
    "    fluctuation_df_xz1314 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    fluctuation = []\n",
    "    df = X_filled_df\n",
    "    for i in df.index:\n",
    "        fluctuation.append( distance.cosine(df.ix[i],mean) )\n",
    "    for i in range(len(df.index.values)):\n",
    "        fluctuation_df_xz1314.ix[df.index.values[i][0],df.index.values[i][1]] = fluctuation[i]\n",
    "#     fluctuation_df_xz1314 = fluctuation_df_xz1314.replace([np.inf], -1.0)\n",
    "#     fluctuation_df_xz1314 = fluctuation_df_xz1314.replace( -1.0, 2.0*fluctuation_df_xz1314.max().max() )\n",
    "    return fluctuation_df_xz1314\n",
    "# fluctuation_df_xz1314 = fluctuationDF_1314(X_filled_df,X_filled_softimpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Distance between nearby regions\n",
    "'''\n",
    "def NNdistDF(X_filled_df):\n",
    "    index = ['A','a-b','B','b-c','C','c-d','D','d-e','E','e-f','F','f-g','G','g-h','H','h-i','I','i-j','J']\n",
    "    columns = ['1','1-2','2','2-3','3','3-4','4','4-5','5','5-6','6','6-7','7']\n",
    "    delta_df_xz13 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    delta_df_xz14 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    rr = 0\n",
    "    df = X_filled_df\n",
    "    for row in range(len(shannon_df_xz13.ix[:,0])-1):\n",
    "        cc = 1\n",
    "        for col in range(len(shannon_df_xz13.ix[0,:])-1):\n",
    "            if totalRC_df_xz13.ix[row,col]!=0 and totalRC_df_xz13.ix[row,col+1]!=0:\n",
    "                ind0 = 'xz13'\n",
    "                ind1 = str(shannon_df_xz13.index.values[row])\n",
    "                ind2 = str(shannon_df_xz13.columns.values[col])\n",
    "                ind3 = str(shannon_df_xz13.columns.values[col+1])\n",
    "                delta_df_xz13.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind0,ind1,ind2],df.ix[ind0,ind1,ind3])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    rr = 1\n",
    "    for row in range(len(shannon_df_xz13.ix[:,0])-1):\n",
    "        cc = 0\n",
    "        for col in range(len(shannon_df_xz13.ix[0,:])-1):\n",
    "            if totalRC_df_xz13.ix[row,col]!=0 and totalRC_df_xz13.ix[row+1,col]!=0:\n",
    "                ind0 = 'xz13'\n",
    "                ind1 = str(shannon_df_xz13.index.values[row])\n",
    "                ind2 = str(shannon_df_xz13.columns.values[col])\n",
    "                delta_df_xz13.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind0,ind1,ind2],df.ix[ind0,ind3,ind2])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    rr = 0\n",
    "    for row in range(len(shannon_df_xz14.ix[:,0])-1):\n",
    "        cc = 1\n",
    "        for col in range(len(shannon_df_xz14.ix[0,:])-1):\n",
    "            if totalRC_df_xz14.ix[row,col]!=0 and totalRC_df_xz14.ix[row,col+1]!=0:\n",
    "                ind0 = 'xz14'\n",
    "                ind1 = str(shannon_df_xz14.index.values[row])\n",
    "                ind2 = str(shannon_df_xz14.columns.values[col])\n",
    "                ind3 = str(shannon_df_xz14.columns.values[col+1])\n",
    "                delta_df_xz14.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind0,ind1,ind2],df.ix[ind0,ind1,ind3])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    rr = 1\n",
    "    for row in range(len(shannon_df_xz14.ix[:,0])-1):\n",
    "        cc = 0\n",
    "        for col in range(len(shannon_df_xz14.ix[0,:])-1):\n",
    "            if totalRC_df_xz14.ix[row,col]!=0 and totalRC_df_xz14.ix[row+1,col]!=0:\n",
    "                ind0 = 'xz14'\n",
    "                ind1 = str(shannon_df_xz14.index.values[row])\n",
    "                ind2 = str(shannon_df_xz14.columns.values[col])\n",
    "                ind3 = str(shannon_df_xz14.index.values[row+1])\n",
    "                delta_df_xz14.ix[index[rr],columns[cc]] = distance.cosine(df.ix[ind0,ind1,ind2],df.ix[ind0,ind3,ind2])\n",
    "            cc = cc+2\n",
    "        rr = rr+2\n",
    "    return delta_df_xz13,delta_df_xz14\n",
    "# delta_df_xz13,delta_df_xz14 = NNdistDF(X_filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fluctuations with respect to the average, measured with KL\n",
    "'''\n",
    "def fluctuationDF(X_filled_df,X_filled_softimpute):\n",
    "    mean = X_filled_softimpute.sum(axis=0)\n",
    "    mean = mean*1.0/mean.sum()\n",
    "    index = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    columns = ['1','2','3','4','5','6','7']\n",
    "    fluctuation_df_xz13 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    fluctuation_df_xz14 = pd.DataFrame(0,index=index, columns=columns)\n",
    "    fluctuation = []\n",
    "    df = X_filled_df\n",
    "    for i in df.index:\n",
    "        fluctuation.append( distance.cosine(df.ix[i],mean) )\n",
    "    for i in range(len(df.index.values)):\n",
    "        if df.index.values[i][0]=='xz13':\n",
    "            fluctuation_df_xz13.ix[df.index.values[i][1],df.index.values[i][2]] = fluctuation[i]\n",
    "        if df.index.values[i][0]=='xz14':\n",
    "            fluctuation_df_xz14.ix[df.index.values[i][1],df.index.values[i][2]] = fluctuation[i]\n",
    "    return fluctuation_df_xz13,fluctuation_df_xz14\n",
    "# fluctuation_df_xz13,fluctuation_df_xz14 = fluctuationDF(X_filled_df,X_filled_softimpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fluctuations between co-localized regions, measured with KL\n",
    "'''\n",
    "def local_fluctuationsDF(X_filled_df):\n",
    "    index = ['A','B','C','D','E','F','G','H','I','J']\n",
    "    columns = ['1','2','3','4','5','6','7']\n",
    "    local_fluctuation_df = pd.DataFrame(0,index=index, columns=columns)\n",
    "    fluctuation = []\n",
    "    df = X_filled_df\n",
    "    if len(df.index.values[0])>2:\n",
    "        for i in range(len(df.index.values)):\n",
    "            if ('xz13',df.index.values[i][1],df.index.values[i][2]) in df.index and ('xz14',df.index.values[i][1],df.index.values[i][2]) in df.index:\n",
    "                pr1 = df.ix['xz13',df.index.values[i][1],df.index.values[i][2]]\n",
    "                pr2 = df.ix['xz14',df.index.values[i][1],df.index.values[i][2]]\n",
    "                local_fluctuation_df.ix[ df.index.values[i][1],df.index.values[i][2] ] = distance.cosine(pr1,pr2)\n",
    "    return local_fluctuation_df\n",
    "# local_fluctuation_df = local_fluctuations(X_filled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
